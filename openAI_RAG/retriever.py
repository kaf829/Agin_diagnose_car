from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
import os
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter  
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import math
load_dotenv()  # â† .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ

PERSIST_DIR = "./vectorstore"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ìˆ˜ë™ ì „ë‹¬


def ingest_pdf(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 200, batch_size: int = 100):
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_documents(documents)

    print(f"ğŸ” ì´ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(chunks)}")

    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

    # âœ… Chroma ë¹ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vectordb = Chroma(
        collection_name="hyundai_manual",
        embedding_function=embeddings,
        persist_directory=PERSIST_DIR
    )

    # âœ… batch ë‹¨ìœ„ë¡œ ì•ˆì „í•˜ê²Œ add_documents í˜¸ì¶œ
    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i:i + batch_size]
        print(f"ğŸ“¦ Embedding ì²­í¬ {i} ~ {i + len(batch_chunks)}")
        vectordb.add_documents(batch_chunks)

    vectordb.persist()
def get_qa_chain():
    if not os.path.exists(PERSIST_DIR):
        raise RuntimeError("â— ë²¡í„° ì €ì¥ì†Œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectordb = Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embeddings
    )

    retriever = vectordb.as_retriever()
    llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=True
    )
    return qa_chain
