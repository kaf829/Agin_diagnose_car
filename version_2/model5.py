from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import gradio as gr
import torch

model_id = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id).to("cpu")  # CPU ëª…ì‹œ

chatbot = pipeline("text-generation", model=model, tokenizer=tokenizer)

def car_repair_bot(user_input):
    prompt = f"""### ì‚¬ìš©ì: {user_input}
### ìë™ì°¨ ì •ë¹„ ì „ë¬¸ê°€:"""
    response = chatbot(prompt, max_new_tokens=256, temperature=0.7)
    return response[0]["generated_text"].split("### ìë™ì°¨ ì •ë¹„ ì „ë¬¸ê°€:")[-1].strip()

iface = gr.Interface(
    fn=car_repair_bot,
    inputs=gr.Textbox(lines=4, label="ì¦ìƒ ë˜ëŠ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"),
    outputs=gr.Textbox(label="ì •ë¹„ ì „ë¬¸ê°€ì˜ ë‹µë³€"),
    title="ğŸš— ìë™ì°¨ ì •ë¹„ ì „ë¬¸ê°€ ì±—ë´‡ (phi-2)",
    description="ìë™ì°¨ ê³ ì¥ ì¦ìƒì´ë‚˜ ì •ë¹„ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì „ë¬¸ê°€ì²˜ëŸ¼ ë‹µë³€í•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤."
)

iface.launch()
